import re
import json
import csv
import time
import random
import logging
from datetime import datetime, timezone
from pathlib import Path
import requests
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# â”€â”€â”€ íƒ€ê²Ÿ ì„¤ì • (ê²€ìƒ‰ í‚¤ì›Œë“œ ê°•í™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TARGETS = [
    {
        "key": "amazon_us",
        "label": "ğŸ‡ºğŸ‡¸ Amazon US",
        "region": "North America",
        "url": "https://www.amazon.com/gp/new-releases/videogames/20972797011/",
        "currency": "USD",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_jp",
        "label": "ğŸ‡¯ğŸ‡µ Amazon JP",
        "region": "Asia",
        "url": "https://www.amazon.co.jp/-/en/gp/new-releases/videogames/8019279051/ref=zg_bs_tab_t_videogames_bsnr",
        "currency": "JPY",
        "search_kw": ["ç´…ã®ç ‚æ¼  -PS5", "ç´…ã®ç ‚æ¼ ", "Crimson Desert"]
    },
    {
        "key": "amazon_uk",
        "label": "ğŸ‡¬ğŸ‡§ Amazon UK",
        "region": "Europe",
        # ë„¤ê°€ ì¤€ ìµœì‹  ì£¼ì†Œ
        "url": "https://www.amazon.co.uk/gp/new-releases/videogames/20862635031/",
        "currency": "GBP",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_de",
        "label": "ğŸ‡©ğŸ‡ª Amazon DE",
        "region": "Europe",
        # ë„¤ê°€ ì¤€ ìµœì‹  ì£¼ì†Œ
        "url": "https://www.amazon.de/gp/new-releases/videogames/20904927031/",
        "currency": "EUR",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_fr",
        "label": "ğŸ‡«ğŸ‡· Amazon FR",
        "region": "Europe",
        # FR ìµœì‹  PS5 ì‹ ì œí’ˆ ID ë°˜ì˜
        "url": "https://www.amazon.fr/gp/new-releases/videogames/20904206031/",
        "currency": "EUR",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_it",
        "label": "ğŸ‡®ğŸ‡¹ Amazon IT",
        "region": "Europe",
        "url": "https://www.amazon.it/gp/new-releases/videogames/20904210031/",
        "currency": "EUR",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_es",
        "label": "ğŸ‡ªğŸ‡¸ Amazon ES",
        "region": "Europe",
        "url": "https://www.amazon.es/gp/new-releases/videogames/20904212031/",
        "currency": "EUR",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_ca",
        "label": "ğŸ‡¨ğŸ‡¦ Amazon CA",
        "region": "North America",
        "url": "https://www.amazon.ca/gp/new-releases/videogames/20995057011/",
        "currency": "CAD",
        "search_kw": ["Crimson Desert"]
    },
    {
        "key": "amazon_au",
        "label": "ğŸ‡¦ğŸ‡º Amazon AU",
        "region": "Oceania",
        "url": "https://www.amazon.com.au/gp/new-releases/videogames/7132145051/",
        "currency": "AUD",
        "search_kw": ["Crimson Desert"]
    },
]

HEADERS_POOL = [
    {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36","Accept-Language":"en-US,en;q=0.9"},
    {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/122.0.0.0","Accept-Language":"en-US,en;q=0.9"}
]

DATA_DIR = Path(__file__).parent.parent / "data"
DATA_DIR.mkdir(exist_ok=True)
FIELDNAMES = ["timestamp","store","label","region","asin","url","rank_overall","rank_console","console_category","price","currency","in_stock","error"]

def scrape_category(cfg: dict) -> dict:
    now = datetime.now(timezone.utc).isoformat()
    r = dict(timestamp=now, store=cfg["key"], label=cfg["label"], region=cfg["region"],
             asin=None, url=cfg["url"], rank_overall=None, rank_console=None,
             console_category="PS5 Games", price=None, currency=cfg["currency"], in_stock=None, error=None)

    try:
        resp = requests.get(cfg["url"], headers=random.choice(HEADERS_POOL), timeout=30)
        if resp.status_code == 403:
            r["error"] = "Blocked by Amazon (403)"
            return r
            
        soup = BeautifulSoup(resp.text, "html.parser")

        # 1. ì•„ë§ˆì¡´ ì‹ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ì˜ ë‹¤ì–‘í•œ HTML êµ¬ì¡° ëŒ€ì‘
        items = soup.select(".zg-grid-general-faceout, .p13n-grid-content, [id^='p13n-asin-index-']")
        
        if not items:
            # êµ¬ì¡°ê°€ ì•„ì˜ˆ ë‹¤ë¥¼ ê²½ìš° ë°±ì—… íƒìƒ‰
            items = soup.find_all("div", {"id": "gridItemRoot"})

        found = False
        for idx, item in enumerate(items, 1):
            text_content = item.get_text(" ", strip=True)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            if any(kw.lower() in text_content.lower() for kw in cfg["search_kw"]):
                r["rank_console"] = idx
                
                # ê°€ê²© ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
                price_el = item.select_one(".p13n-sc-price, .a-color-price, ._cDEBy_price_2u01n")
                if price_el:
                    r["price"] = re.sub(r'[^\d.]', '', price_el.get_text())
                
                r["in_stock"] = True
                found = True
                break
        
        if not found:
            r["error"] = "Not in top list"
            # ë””ë²„ê¹…ìš©: ë¦¬ìŠ¤íŠ¸ ì²« ë²ˆì§¸ ìƒí’ˆ ì´ë¦„ ì¶œë ¥
            if items:
                first_item = items[0].get_text(" ", strip=True)[:30]
                logger.info(f"[{cfg['key']}] Not found. Top #1 is: {first_item}...")

    except Exception as e:
        r["error"] = str(e)
        logger.error(f"[{cfg['key']}] Error: {e}")

    return r

def save(results: list):
    csv_path = DATA_DIR / "rankings.csv"
    exists = csv_path.exists()
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=FIELDNAMES)
        if not exists: w.writeheader()
        w.writerows(results)
    
    with open(DATA_DIR / "latest.json", "w", encoding="utf-8") as f:
        json.dump({"updated_at": datetime.now(timezone.utc).isoformat(), "results": results}, f, ensure_ascii=False, indent=2)

def run():
    results = []
    for cfg in TARGETS:
        logger.info(f"Scraping {cfg['label']}...")
        results.append(scrape_category(cfg))
        time.sleep(random.uniform(7, 12)) # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ì§€ì—° ì‹œê°„ ì¦ê°€
    save(results)

if __name__ == "__main__":
    run()
